{
 "metadata": {
  "name": "",
  "signature": "sha256:b8d2609d03f4b1d5c7ae3f629656b9241b0bc88e6a2bde782b46eb803d71b144"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Unit Experiment for Lattice2seg\n",
      "\n",
      "\n",
      "There are two units/components in the struct2seg project\n",
      "1. **struct4wordhood**: recognizing out-of-vocabulary words via #MergeActionPrediction model, a discriminative word-structure parsing model\n",
      "2. **lattice2seg**: use a given word list to construct a *lattice* that represent all possible segmentations, which reduce the word segmentaiton as *disambiguation* problem. The current disambiguation is accomplished via word-based segmentation model that adopt *viterbi search* and using word-bigram features.\n",
      "\n",
      "As *struct2wordhood* has been evaluated in ../parse4wordhood sub-project. We focus on evaluating the *lattice2seg* component\n",
      "\n",
      "\n",
      "\n",
      "### Settings\n",
      "\n",
      "---------------------------------------------------------\n",
      "#### Setting 1 \n",
      "> perfect lexicon for train.corpus, train.word + CRF_predict.lexicon for test.corpus\n",
      "\n",
      "**Training**: Gen wordlist for train.corpus --> Build lattice for train --> Train Model \n",
      "\n",
      "- gen word lsit for train: simply use *train.word* to construct the lattice. (Ideally this should be done in a stacked generalization manner, but the current way is a fast approximation, which may lead to search errors)\n",
      "\n",
      "- build lattice for train: use lattice_build.py to build *trian.lattice*\n",
      "\n",
      "- Train model: \n",
      "  - process *train.lattice* and extract each instance of the form <previous_word, word, incoming_char> using feature_gen.py. \n",
      "  - If this instance is on the path of gold-standard segmentation, label it as \"+\", otherwise label it as \"-\".\n",
      "  - use MaxEnt (or whatever classifier has probabilistic output) to train the model, get **score.model**\n",
      "\n",
      "\n",
      "**Testing**: Gen wordlist for test --> Build lattice for test --> Decode the lattice\n",
      "- gen word list for test: the word list of the test corpus is made up by the following\n",
      "  - train.word\n",
      "  - build a standard CRF seg model from train.corpus, then apply to test.corpus finally collect its crf_predict.word\n",
      "\n",
      "- build lattice for test. use lattice_build.py\n",
      "\n",
      "- Decode the lattice: use viterbi_search.py to search the lattice, which calls bigram_score to get prediciton from *score.model*, and viterbi_search.py just get the best result by comparing scores.\n",
      "\n",
      "\n",
      "\n",
      "##### Experiment code need to implement\n",
      "1. lattice2seg_gen_instance_to_train_score_model.py: implement: gen wordlist for train.corpus, build train.lattice and train model part before feeding to maxent\n",
      "\n",
      "2. score_function, put in viterbi_search.py, that load the maxent model and some code to compute maxent output using given features.  Also need to change viterbi_search's API for score_function\n",
      "\n",
      "3. lattice2seg_test: input: test, train.word, crf_predict(test).word, build test.lattice, and decode the lattice using viterbi_search.py and then convert to segmentation result.\n",
      "\n",
      "use official ./score perl script to compare the result with CRF.\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------\n",
      "#### setting 2:\n",
      ">stacked generalization based procedure to get  train.word\n",
      "\n",
      "Each time, get N-1/N part of train.corpus, known as train.sample_a  train a model (e.g. CRF), make the prediction on the remaining 1/N part of train.corpus, known as train.sample_b, to get 1/N part of \"predicted\" word in CRF\n",
      "\n",
      "In such N-cross validation manner, the \"predicted\" word for train.corpus will be generated, which is better simulation of how the test.word is generated in testing time.\n",
      "\n",
      "##### Code to implement\n",
      "\n",
      "1. N-CV-corpus split\n",
      "2. \"map\" script_parallel experiment\n",
      "3. \"reduce\" result merge"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Experiment Setting 1:\n",
      "\n",
      "#### code 1,2 completed, now train the model on @cleo server\n",
      "> [nnsmj01@cleo exp_l2s]$ maxent -m  train.set1.model  train.ctb5.score.ins -v\n",
      "\n",
      "Result:\n",
      "iter  eval     loglikelihood  training accuracy   heldout accuracy\n",
      "\n",
      "  0      1      -6.931472E-01     67.231%            N/A\n",
      "  \n",
      " 10     11      -3.521837E-01     86.892%            N/A\n",
      " \n",
      " 20     23      -2.685207E-01     89.876%            N/A\n",
      " \n",
      " 30     33      -2.331152E-01     91.288%            N/A\n",
      " \n",
      " \n",
      "Now we try 80 iteration and also  5-fold cross validation\n",
      "\n",
      "> maxent -m  train.set1.i80.n5.model  train.ctb5.score.ins -v 0 -i 80 -n 5\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}