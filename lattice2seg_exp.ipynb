{
 "metadata": {
  "name": "",
  "signature": "sha256:c0774b94a658784ec6878c52b06d14fd932eea2d25262acb8dc0e70b70c99443"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Unit Experiment for Lattice2seg\n",
      "\n",
      "\n",
      "There are two units/components in the struct2seg project\n",
      "1. **struct4wordhood**: recognizing out-of-vocabulary words via #MergeActionPrediction model, a discriminative word-structure parsing model\n",
      "2. **lattice2seg**: use a given word list to construct a *lattice* that represent all possible segmentations, which reduce the word segmentaiton as *disambiguation* problem. The current disambiguation is accomplished via word-based segmentation model that adopt *viterbi search* and using word-bigram features.\n",
      "\n",
      "As *struct2wordhood* has been evaluated in ../parse4wordhood sub-project. We focus on evaluating the *lattice2seg* component\n",
      "\n",
      "\n",
      "\n",
      "### Settings\n",
      "\n",
      "---------------------------------------------------------\n",
      "#### Setting 1 \n",
      "> perfect lexicon for train.corpus, train.word + CRF_predict.lexicon for test.corpus\n",
      "\n",
      "**Training**: Gen wordlist for train.corpus --> Build lattice for train --> Train Model \n",
      "\n",
      "- gen word lsit for train: simply use *train.word* to construct the lattice. (Ideally this should be done in a stacked generalization manner, but the current way is a fast approximation, which may lead to search errors)\n",
      "\n",
      "- build lattice for train: use lattice_build.py to build *trian.lattice*\n",
      "\n",
      "- Train model: \n",
      "  - process *train.lattice* and extract each instance of the form <previous_word, word, incoming_char> using feature_gen.py. \n",
      "  - If this instance is on the path of gold-standard segmentation, label it as \"+\", otherwise label it as \"-\".\n",
      "  - use MaxEnt (or whatever classifier has probabilistic output) to train the model, get **score.model**\n",
      "\n",
      "\n",
      "**Testing**: Gen wordlist for test --> Build lattice for test --> Decode the lattice\n",
      "- gen word list for test: the word list of the test corpus is made up by the following\n",
      "  - train.word\n",
      "  - build a standard CRF seg model from train.corpus, then apply to test.corpus finally collect its crf_predict.word\n",
      "\n",
      "- build lattice for test. use lattice_build.py\n",
      "\n",
      "- Decode the lattice: use viterbi_search.py to search the lattice, which calls bigram_score to get prediciton from *score.model*, and viterbi_search.py just get the best result by comparing scores.\n",
      "\n",
      "\n",
      "\n",
      "##### Experiment code need to implement\n",
      "1. lattice2seg_gen_instance_to_train_score_model.py: implement: gen wordlist for train.corpus, build train.lattice and train model part before feeding to maxent\n",
      "\n",
      "2. score_function, put in viterbi_search.py, that load the maxent model and some code to compute maxent output using given features.  Also need to change viterbi_search's API for score_function\n",
      "\n",
      "3. lattice2seg_test: input: test, train.word, crf_predict(test).word, build test.lattice, and decode the lattice using viterbi_search.py and then convert to segmentation result.\n",
      "\n",
      "use official ./score perl script to compare the result with CRF.\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------\n",
      "#### setting 2:\n",
      ">stacked generalization based procedure to get  train.word\n",
      "\n",
      "Each time, get N-1/N part of train.corpus, known as train.sample_a  train a model (e.g. CRF), make the prediction on the remaining 1/N part of train.corpus, known as train.sample_b, to get 1/N part of \"predicted\" word in CRF\n",
      "\n",
      "In such N-cross validation manner, the \"predicted\" word for train.corpus will be generated, which is better simulation of how the test.word is generated in testing time.\n",
      "\n",
      "##### Code to implement\n",
      "\n",
      "1. N-CV-corpus split\n",
      "2. \"map\" script_parallel experiment\n",
      "3. \"reduce\" result merge"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\n",
      "# test exp_l2s_gen_ins_to_train_score_model.py  the main function\n",
      "#\n",
      "import exp_l2s_gen_ins_to_train_score_model\n",
      "\n",
      "path_to_corpus, path_to_instance_file = '../working_data/train.ctb5.seg', '../working_data/tmp.out'\n",
      "instance_list = []\n",
      "corpus, word_list = exp_l2s_gen_ins_to_train_score_model.read_training_corpus(path_to_corpus)\n",
      "print len(corpus), corpus[0], len(word_list), \" / \".join(list(word_list)[:3])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'module' object has no attribute 'read_training_corpus'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-7-4da5a54df651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpath_to_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_instance_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../working_data/train.ctb5.seg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../working_data/tmp.out'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0minstance_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_l2s_gen_ins_to_train_score_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_training_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" / \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'read_training_corpus'"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}