{
 "metadata": {
  "name": "",
  "signature": "sha256:761bc23d7c03eebeba66671063c1db9b5ee1481949d3818a5882cdca134f17bf"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Word Lattice Gen and Traversal\n",
      "\n",
      "##Initial Design (*out-of-date! See the New design in next text/markdown cell*)\n",
      "###Input \n",
      "1. a set of words (unicode string)\n",
      "2. a list of sentences in the form of list/string of characters\n",
      "\n",
      "###Output:\n",
      "~~a representation of word lattice. Current design:~~\n",
      "~~- a **sent_list** to represent the sentence, each index correspond to a char position. sent of *n* char has *n+1* positions~~\n",
      "~~- at each index of sent_list, two lists are kept:~~\n",
      "  - ~~**forward_word_table** a *dict*, key=word_length, value= set of words that *starts* at the index poistion of the sentence that are of length of word_length~~ \uff08turns out that only backward_word_table is needed for the DP/Viterbi search\uff09\n",
      "~~  - **backward_word_table** a *dict*, similar to forward_word_table, except that it records words that *ends* at this position~~\n",
      "~~> - usage scenario: extracting words at specific position and their *word-level* context:at each position x, a word candidate *candy* of length n can found in sent_list[x].forward_word_table[n].~~\n",
      "~~>  Its *left word context* can be found in sent_list[x].backword_word_table[?] and its *right word context* can be found in sent_list[x+n].forward_word_table[?]~~\n",
      "\n",
      "\n",
      "----------------------------\n",
      "##Improved Design (see lattice_build.py)\n",
      "\n",
      "\n",
      "\n",
      "### Input: same\n",
      "\n",
      "### Output:\n",
      "- A representation of the lattice for each sentence **lattic_list**, which is a list of lattice\n",
      "  - A **lattice** is a list of dictionaries. *lattice[i]* denotes all the *word bigram* that ends at the position *i* of the sentence\n",
      "  - the dictionary is called **backward_bigram_table**\n",
      "    - *key*: start_index_of_current_word\n",
      "    - *value*: 4-tuple (start_index_of_previous_word, start_index_of_current_word, previous_word_str, current_word_str)\n",
      "    - a 4-tuple **represents**: previous_word==sent[start_index_of_previous_word:end_index_of_previous_word] and current_word=[ start_index_of_current_word:end_index_of_current_word]\n",
      "    - note: a.index are kept for *search algorithm*, while word str are kept for fast feature generation   b. Python convention, the span means [star: end), c. end_index_of_previous word == start_index_of_current_word; \n",
      "  - another dictionary called **forward_word_table**, each of which is asociated with position *i*, it denotes all the words that *start* from that position\n",
      "    - *key*: end_index_of_current_word\n",
      "    - *value*: current_word (that starts at index i and ends at index end_index_of_current_word)\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\n",
      "# (out-dated! experimental!)function for build word lattice\n",
      "#\n",
      "def lattice_build(word_list, lines_of_sentences):\n",
      "    if not type(word_list) is set:\n",
      "        word_list=set(word_list)\n",
      "    \n",
      "    lattice_list=[]\n",
      "    \n",
      "    for sent in lines_of_sentences:\n",
      "        lattice=[]\n",
      "        print \"\\n\\n------ sent=\",  \" \".join(sent), \"------\"\n",
      "        for i in range(len(sent)):\n",
      "            print '\\n\\n==>position ',i\n",
      "            sent_len=len(sent)\n",
      "            forward_word_table={k:u\"\".join(sent[i:i+k]) for k in range(1,sent_len+1-i) if u\"\".join(sent[i:i+k]) in word_list}\n",
      "            backward_word_table={k:u\"\".join(sent[i-k:i]) for k in range(i+1) if u\"\".join(sent[i-k:i]) in word_list}\n",
      "            lattice.append((forward_word_table, backward_word_table))\n",
      "            \n",
      "            fwd_t, bwd_t=forward_word_table, backward_word_table\n",
      "            print '\\nfwd_t:'\n",
      "            for l in fwd_t:\n",
      "                print l, fwd_t[l]\n",
      "            print '\\nbwd_t:'\n",
      "            for l in bwd_t:\n",
      "                print l, bwd_t[l]\n",
      "            print '===========\\n'        \n",
      "        \n",
      "        lattice_list.append(lattice)\n",
      "    \n",
      "    return lattice_list\n",
      "\n",
      "#we decide that we use sent representation WITHOUT star/end symbol\n",
      "lines_of_sent=[['#START#','a','b','c','a','#END#'], ['a','b','c','a']]  #with and without special symbol for start/end of sentence\n",
      "\n",
      "word_list=['#START#','ab','bca','ca', '#END#']\n",
      "\n",
      "lattice_list=lattice_build(word_list, lines_of_sent)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Search/Decoding based on Viterbi Algorithm\n",
      "\n",
      "##Input & Output\n",
      "### Input\n",
      "- for each sentence, there is a *lattice*, which is tuple of (forward_unigram_lattice, backward_bigram_lattice)\n",
      "  - *forward_unigram_lattice*  keeps forward_word_table for position 0...n-1, where n==len(sent)\n",
      "  - *backward_bigram_lattice*  keeps backward_bigram_table for position 1...n, where n==len(sent)\n",
      "### Output\n",
      "- the \"best\" word sequence that covers the original character sequence.\n",
      "> the scoring is based on the evaluation of word bigrams (or unigrams), with optional one word look-ahead, with regard to utility function.\n",
      "\n",
      "\n",
      "## Algorithm\n",
      "The search incrementally moves from position 0 to n of the sentence. It keeps **best_seq[i]**, the score of the best word sequences (for e.g. word segmentation) until position *i*, where i=0..n.\n",
      "\n",
      "-*best_seq[i]* is a dict \n",
      "  - key: start_index(_of_a_last_word_of_the_seq)\n",
      "  - value: (score_of_seq, back_trace_index)\n",
      "\n",
      "Note: since at each step a *bigram* is considered, it is necessary to keep the scores the \"best\" sequence that ends with each of the possible \"last word\"\n",
      "\n",
      "In other words, best_seq[i][j] keeps the score of the best sequence until position *i* with the last word being sent[j:i]\n",
      "\n",
      "\n",
      "~~- *best_seq[i]=(**hash_table_of_word_seq**, star_index_last_word, socre_of_the_seq)*~~\n",
      "~~  - For simplicity of the algorithm description, we also use the notation *best_seq[i].last_word* = sent[start_index_last_word:end_index_last_word], where end_index_last_word == i~~\n",
      "\n",
      "\n",
      "Since it's a dynamic programming algorithm, it can be described with basis and inductive steps.\n",
      "\n",
      "  ### Basis: *i* = 0\n",
      "best_seq[0]={None:1.0}\n",
      "i.e. best_seq[0][None]=1.0 \n",
      "\n",
      "\n",
      "  ### Inductive steps:\n",
      "compute best_seq[i]\n",
      "\n",
      "for each 0<j<i: \n",
      "    update best_seq[i][j].score = best of the following\n",
      "        bigram(k, j, i).score + best_seq[i-j][k], where 0<k<j\n",
      "    update best_seq[i][j].back_trace_index as the *k* that corresponds to best_seq[i][j].score\n",
      "\n",
      "\n",
      "The last word in a  \"best sequence\" in best_seq[k] much match with the previous_word in one of the bigrams (k, j, i) (i.e. sent[k:j]) kept in the backward_bigram_table for the sentence position *i* (the table is kept in backward_bigram_lattice[i])\n",
      "\n",
      "Formally, the criteria are the followings:\n",
      "\n",
      "  - best_seq[j] is non-empty, and best_seq[j][k] is non-empty\n",
      "  - for the backward_bigram_table in backward_bigram_lattice[i],  backward_bigram_table[i-k] is an non-emtpy set\n",
      "  - furthermore, this set contains the tuple (start_index_of_previous_word, start_index_of_current_word, previous_word_str, current_word_str) such that:         \n",
      "    1. start_index_of_previous_word==best_seq[i-k].start_index_of_last_word and \n",
      "    2. start_index_of_current_word==i-k (i.e. end_index_of_last_word)\n",
      "\n",
      "\n",
      "  ### Final result:\n",
      "best_seq[n], if non-empty, it contains the best word sequence"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\n",
      "# !! Experimental code for search algorithm (without lookahead)\n",
      "#\n",
      "\n",
      "from lattice_build import lattice_build_bigram\n",
      "\n",
      "lines_of_sent = [u\"\u6750 \u6599 \u5229 \u7528 \u7387 \u9ad8\".split()]  # with and without special symbol for start/end of sentence\n",
      "word_list = [u'\u6750\u6599', u'\u5229\u7528', u'\u5229\u7528\u7387', u'\u7387', u'\u9ad8']\n",
      "\n",
      "lattice_list=lattice_build_bigram(word_list, lines_of_sent)\n",
      "\n",
      "def viterbi_search (backward_lattice, max_word_len):\n",
      "    best_seq=[]\n",
      "    \n",
      "    #########\n",
      "    # basis #\n",
      "    #########\n",
      "    init_word_seq, init_star_index_last_word, init_score= ['#START#'], None, 1.0\n",
      "    best_seq.append((initial_word_seq, init_score))\n",
      "    \n",
      "    \n",
      "    #################\n",
      "    # Inductive Step#\n",
      "    #################\n",
      "    \n",
      "    for i in range (1, len(backward_lattice)+1):\n",
      "        \n",
      "        \n",
      "        \n",
      "    \n",
      "    \n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "building lattice for------ sent= \u6750 \u6599 \u5229 \u7528 \u7387 \u9ad8 ------\n",
        "p_start, current_start, previous_word, current_word==> 0 2 \u6750\u6599 \u5229\u7528 (check= \u6750\u6599 \u5229\u7528 )\n",
        "p_start, current_start, previous_word, current_word==> 0 2 \u6750\u6599 \u5229\u7528\u7387 (check= \u6750\u6599 \u5229\u7528\u7387 )\n",
        "p_start, current_start, previous_word, current_word==> 2 4 \u5229\u7528 \u7387 (check= \u5229\u7528 \u7387 )\n",
        "p_start, current_start, previous_word, current_word==> 2 5 \u5229\u7528\u7387 \u9ad8 (check= \u5229\u7528\u7387 \u9ad8 )\n",
        "p_start, current_start, previous_word, current_word==> 4 5 \u7387 \u9ad8 (check= \u7387 \u9ad8 )\n",
        "length of forward_lattice= 6 ; len of backward_lattice= 6\n",
        "\n",
        "building lattice for------ sent= \u6750 \u6599 \u5229 \u7528 \u7387 \u9ad8 ------\n",
        "p_start, current_start, previous_word, current_word==> 0 2 \u6750\u6599 \u5229\u7528 (check= \u6750\u6599 \u5229\u7528 )\n",
        "p_start, current_start, previous_word, current_word==> 0 2 \u6750\u6599 \u5229\u7528\u7387 (check= \u6750\u6599 \u5229\u7528\u7387 )\n",
        "p_start, current_start, previous_word, current_word==> 2 4 \u5229\u7528 \u7387 (check= \u5229\u7528 \u7387 )\n",
        "p_start, current_start, previous_word, current_word==> 2 5 \u5229\u7528\u7387 \u9ad8 (check= \u5229\u7528\u7387 \u9ad8 )\n",
        "p_start, current_start, previous_word, current_word==> 4 5 \u7387 \u9ad8 (check= \u7387 \u9ad8 )\n",
        "length of forward_lattice= 6 ; len of backward_lattice= 6\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}